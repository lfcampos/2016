---
title: "Inference"
output: html_document
---

# Inference 

We are going to have a competition. There is a prize.

You have to guess the proportion of blue beads in the jar. You can take a sample (with replacement)
from the jar here:

[https://dgrtwo.shinyapps.io/urn_drawing/](https://dgrtwo.shinyapps.io/urn_drawing/)

Each sample will cost you 10 cents ($0.10). So if you take a sample size of 100 you will have to pay me $10 to collect your prize. 

Once you take your sample, please enter your guess, sample size, and an interval size. We will create an interval of this size with your guess in the middle. If the true percentage is not in your interval you are eliminated. The size of the interval will serve as a tie breaker (smaller wins).

[http://goo.gl/forms/5bv4cRQWKA](http://goo.gl/forms/5bv4cRQWKA)

The deadline to enter the competition is Monday February 22, 2016 at 9:00 AM.


## Introduction

This chapter introduces the statistical concepts necessary to understand margins of errors, p-values and confidence intervals. These terms are ubiquitous in data science. Let's use polls as an example:


```{r, echo=FALSE, cache=TRUE, warning=FALSE, message=FALSE}
library(ggplot2)
library(dplyr)
library(tidyr)
library(pollstR)

theme_set(theme_bw())

race2012 <- pollstr_polls(topic = '2012-president', after= as.Date("2012-11-3"), max_pages = Inf)

polls <- race2012$questions %>% 
  filter(topic=="2012-president" & state=="US") %>% 
  select(choice, value, margin_of_error, observations, id) %>% 
  filter(choice %in% c("Obama","Romney") & !is.na(margin_of_error)) %>% 
  spread(choice, value) %>% 
  left_join(select(race2012$polls, id, pollster, method), by="id") %>%
  filter(method!="Internet") %>%
  mutate(diff= Obama - Romney) %>%
  select(pollster, diff, Obama, Romney, margin_of_error, observations)

arrange(polls,diff) %>% rename( n=observations) %>% 
  mutate(pollster=ifelse(pollster=="PPP (D-Americans United for Change)","PPP",pollster))
```

What does _margin of error_ mean? The first step is define and understand random variables. 
To do this, we will generate our own imaginary election with 1 million voters of which proportion $p$ are democrats and $1-p$ are republicans. To keep it interesting we will keep generate $p$ at random and not peek.

```{r}
n <- 10^6 ##number of voters
set.seed(1) ##so we all get the same answer
##pick a number between 0.45 and 0.55 (don't peek!):
p <- sample( seq(0.45,0.55,0.001),1) 
x <- rep( c("D","R"), n*c( p, 1-p))
x <- sample(x) ##not necessary, but shuffle them
```

The population is now fixed. There is a true proportion $p$ but we don't know it.

One election day we will do the following to decide who wins (don't ruin the fun by doing it now!):

```{r, eval=FALSE}
prop.table(table(x))
```

Pollsters try to _estimate_ $p$ but asking 1 million people and actually unnecessary so instead. They take a poll. To do this they take a random sample. They pick $N$ random voter phone numbers, call, and ask. Assuming everybody answers and every voter has a phone, we can mimic a random sample of 100 people with:

```{r}
poll <- sample(x, 25, replace = TRUE)
```

The pollster then looks at the proportion of democrats in the sample and uses this information to predict $p$. In Statistics we say try to _estimate_ p. 

```{r}
table(poll)
```

So our poll predicts a democrat win! Is this a good _estiamte_? We will see how powerful mathematical statistics is at informing us about exactly how good it is.

Notation: we use lower case $x$ for the population of all voters and capital letters $X$ for the random sample.

## Random Variables

Random variable are outcomes of random process. The number of democrats out of the 25 we picked at random is an example. Let's repeat the exercise above a few times. Let's run a few other polls and see what happens:

```{r}
X <- sample(x, 25, replace = TRUE)
sum( X=="D")

X <- sample(x, 25, replace = TRUE)
sum( X=="D")

X <- sample(x, 25, replace = TRUE)
sum( X=="D")
```

Note how the observed number varies.  We refer to this as _random_ or _chance_ variation. How does this random variable relate to our quantity of interest $p$? Statistical theory has a lot to teach about this and is the main tool used by pollsters and poll aggregators.

## Sampling Models

Many procedures studied and used by data scientist can be modeled quite well as a sum of draws from a jar. For example, we modeled the process of polling likely voters as drawing 0s (republicans) and 1s (republicans) from a jar. To be more specific we can turn the `x` into 0s and 1s:

```{r}
x <- as.numeric(x=="D")
sample(x, 25, replace = TRUE)
```

Many other examples can be constructed. For example to model our winning after betting $1 on red (on a roulette wheel) 10 times we can use:

```{r}
color <- rep(c("Black","Red","Green"), c(18,18,2))
X <- sample( ifelse( color=="Red", 1,-1),  10, replace = TRUE)
sum(X)
```

Because we know the proportions we can do this a bit quicker by simply using:

```{r}
X <- sample( c(-1,1), 10, replace = TRUE, prob=c(10/19, 9/19))
sum(X)
```

Assessment: Develop a sampling model for betting on 7 on roulette:

## The Expected Value and Standard Error

Now that we have described sampling models, now we can describe properties of the sum of draws. The first important concept to learn is the _expected value_. The random variable will vary around this expected value. 
The _standard error_ gives us an idea of how the size of the variation around the expected value.

The expected value of the sum of draws is the 

$$\mbox{number of draws } \times \mbox{ average of the numbers in the jar}$$

Using the definition of average:

$$\mu = \frac{1}{n}\sum_{i=1}^n x_i$$

We can see that the average of values inside a jar with 0s and 1s is the number of 1s divided by $n$. This is the proportion of 1s. In general if a jar has two values $a$ and $b$ with proportions $p$ and $(1-p)$ respectively, the average is $ap + b(1-p)$.

Assessment: What are the averages for the roulette examples? [http://goo.gl/forms/XNIW22JSsc](http://goo.gl/forms/XNIW22JSsc)
- Betting on black 
- Betting on 7

In our example with the election we know that the average of the box is $p$, we just don't know what $p$ is just yet.

**If our draws are independent**, then the standard error of our sum is 

$$\sqrt{\mbox{number of draws }} \times \mbox{ standard deviation of the numbers in the jar}$$

Using the definition of standard deviation

$$ \sigma = \sqrt{ \frac{1}{n} \sum_i^n (x_i - \mu)^2} $$

we can derive, with a bit of math, that if a jar has two values $a$ and $b$ with proportions $p$ and $(1-p)$ respectively, the standard deviation is $\mid b - a \mid \sqrt{p(1-p)}$.

Assessment: What are the expected value and standard errors of the number of democrats. Hint: The answers are functions of $p$, the proportion of democrats. [http://goo.gl/forms/70Um4fp5Pn](http://goo.gl/forms/70Um4fp5Pn)

Let $S$ be the number of democrats in our sample, or the sum of the 25 draws. Our expected value is 

$$\mbox{E}(S) = 25p$$

and standard error is 

$$\mbox{SE}(S) = \sqrt{25 p (1-p)}$$

Two more intuitive results. If we divide $S$ by constant, the expected value and standard error are also divided by this constant. This implies that 

Now we now that the expected value of the proportion in our sample $S/25$, has expected value $p$. 

$$\mbox{E}(S/25) = p$$

which implies that $S/25$ will be $p$ plus some chance error. 

The SE, 

$$\mbox{SE}(S/25) = \sqrt{p(1-p)} / \sqrt{N}$$

gives us an idea of how large the error is.


We do notice that by making $N$ larger we can make our standard error smaller. The _Law of Large Numbers_ tells us that the bigger $N$ the closer the sample average gets to the average of the jar which is the quantity we want to estimate.


### Estimates

Beacuse $S/25$ is $p$ plus or minus some chance error we use it as our _estimate_ of $p$. We usually put hats on top of our estimates: $\hat{p}$. We now now that when $N$ is large $\hat{p} \approx p$.


Suppose that after we take our poll we want to give a best guess for $p$. One way we could do this is to report the observed value and it's standard error. Unfortunately, we need to know $p$ to report an expected value. It turns out that plugging in the observed proportion,  works out pretty well. So we can report:

```{r}
p_hat <- mean(poll=="D")
cat("Our estimate of the percent of decomrats is",p_hat,
    "plus or minus", round( sqrt(p_hat*(1-p_hat)/25), 2))
```

Which is not terribly useful. However, now we know that we can be more precise by taking a larger poll.

Assessment: If $p=0.5$, how large should the poll be to have a standard error of 2% or less ? Then take a poll and report expected value and standard error [http://goo.gl/forms/R9Ke1UfHBB](http://goo.gl/forms/R9Ke1UfHBB)


## Probability Distribution for Random Variables

The plus or minus statements above are not very precise. Is it possible to say more? Can we, for example, compute the probability that $\hat{p}$ is within 1% of the actual $p$ ? It turns out that we can. First let's start with a Monte Carlo simulation. 

Assessment: Repeat the exercise of taking a poll of 625 likely voters, 10,000 times. Without peaking at $p$ study the distribution of $\hat{p}-p$. Have we seen this distribution before? How often was our error smaller larger than 0.02? [http://goo.gl/forms/JqyeuvvJiR](http://goo.gl/forms/JqyeuvvJiR)

The distribution we see here is the _probability distribution_ of our random variable. Knowing this distribution will be extremely helpful because we can, for example, report the probability that our error is smaller than a particular quantity. In the next section we describe a powerful mathematical result that helps simplify this.


## Central Limit Theorem

We can see that the distribution of $\hat{p}$ is very well approximated with the normal distribution. 

```{r}
qqnorm(error)
qqline(error)
```

The fact that the distribution is centered at 0 confirms that the expected value of $\hat{p}$ is $p$. The fact that the standard deviation of the errors is 

```{r}
sqrt(mean( (error)^2))
```
confirms that the standard error is in fact 0.02. Furthermore, we could have predicted that 32% of errors were larger than 0.02 using the normal approximation. What proportion of the normal curve is more than 1 SD away from average?

```{r}
##compare
mean(abs(error) > 0.02)
##to
pnorm(-1) + (1-pnorm(1))
```

Notice that the Monte Carlo simulation is like conducting 10,000 polls. This of course is practically impossible. But the mathematical theory saves us from having to do this! Without just one poll we are able to say that our estimate is $\hat{p}$ and there is 32% chance that our error is larger than 2%. 

## Confidence Intervals

If we create intervals that miss the target 32% of the time, we will not be considered good forecasters. We can always make less bold predictions, such as "the percent of democrats will be between 0% and 100%" but that will not help our reputation either. Can we find a better balance? 

Because there is chance variation, it is impossible to hit the target 100% of time unless we state the obvious "between 0% and 100%". So let's compromise a bit. If we hit the target 95% of the time, chances are we will be considered good forecasters. Can we use the theory above to construct such interval?

What we want to do is use the result of our poll to construct an interval, say, $[A,B]$ such that 

$$\mbox{Pr}(A \leq p \mbox{ and } B \geq p) \geq 0.95$$

we want to make this interval as small as possible. How do we choose $A$ and $B$? 

We know $\hat{p}$ follows a normal distribution with expected value $p$ and standard error approximately equal to $\sqrt{\hat{p} (1-\hat{p})}/\sqrt{N}$. This implies that the following random variable

$$Z = \sqrt{N}\frac{ \hat{p} - p}{\sqrt{\hat{p} (1-\hat{p})}}$$

is approximately normal with expected 0 and standard deviation 1.

Assessment: What value can we make $z$ so that 
$\mbox{Pr}(\mid Z \mid \leq z) = 0.95$ ? [http://goo.gl/forms/D6ngZ4Effd](http://goo.gl/forms/D6ngZ4Effd)


So we have our interval. We need to actually define it in terms of $\hat{p}$, $N$, and $z$ because these are the only quantity we can actually construct. 

Assessment: 
How do we rewrite the event $$ Z \leq z$$ so that it can be written as an interval of quantities we know.


This interval has a 95% chance of including the true $p$

Did we actually make a correct prediction? It's election night and we we will find out

```{r}
ci <- p_hat + c(-1,1)*qnorm(0.975)*sqrt(p_hat*(1-p_hat))/sqrt(N)
p 
ci[1] <= p & ci[2] >= p
```

Assessment: Construct a 99% confidence interval. Is it more or less specific than the 95%?

### Confidence Intervals: Monte Carlo Simulaion 

Note that Nate Silver stated that Obama had a 90% chance of wining. Was this a confidence interval?

The description we have given up to know says nothing about the probability of winnings. In fact, the statement does not even make sense because $p$ is fixed. It is not random. We should not make probability statements. 

Later we will learn about Bayesian statistics were data-based statements such as "Obama has a 90% chance of winning" make sense. Here we clarify how we interpret margins of error and confidence intervals.

The 95% confidence interval 

```{r}
p_hat + c(-1,1)*qnorm(0.995)*sqrt(p_hat*(1-p_hat))/sqrt(N)
```

is random because $p_hat$ is random.  Let's use a Monte Carlo simulation to see this:

```{r, message=FALSE}
library(dplyr)
library(ggplot2)
N <- 625
B <- 100
tab  <- replicate(B,{
  X <- sample(x, N, replace = TRUE)
  p_hat <- mean(X)
  ci <- p_hat + c(-1,1)*qnorm(0.975)*sqrt(p_hat*(1-p_hat))/sqrt(N)
  hit <- ci[1] <= p  & ci[2] >= p
  c(p_hat,ci,hit)
})

tab <- data.frame(poll=1:ncol(tab), t(tab))
names(tab)<-c("poll","estimate","low","high","hit")
tab <- mutate(tab, hit=ifelse(hit, "Hit","Miss") )
ggplot(tab, aes(poll,estimate,ymin=low,ymax=high,col=hit)) +geom_point(aes())+geom_errorbar() + coord_flip() + geom_hline(yintercept = p)
```

Notice how the interval is different after each poll: it's random. So the 95% relates to the probability that this random interval falls on top of $p$, not that $p<.50$ or whatever other statements related to the probability of winning. In the figure above, that shows the interval for 100 polls, you can see that interval fall on $p$ about 95% of the time.


### Power

Pollsters are not successful for providing correct confidence intervals but rather for predicting who will win. Unfortunately, our confidence interval included $0.5$ so if forced to declare a winner, we would have to say it was a "toss-up". A problem with our poll results is that, given the sample size and the value of $p$, we would have to sacrifice on the probability of an incorrect call to create an interval that does not include 50. In this particular case, the reported margin of error would have had to be about 3.5%
 
Assessment: What percent of confidence intervals with margins of error of 0.03 would correctly predict the election? Check with a Monte Carlo simulation. [http://goo.gl/forms/CIrlMKhtTq](http://goo.gl/forms/CIrlMKhtTq)

Define 
$$ \delta = 0.035$$
We want to know
$$\mbox{Pr}( - \delta < \mid p - \hat{p} \mid < \delta)$$
$$\mbox{Pr}( | Z | < \sqrt{N} \delta/\sqrt{p(1-p)})$$

Because $Z$ is approximately standard normal this probability is approximately
```{r}
a = sqrt(N)*.035/sqrt(p_hat*(1-p_hat))
pnorm(a)-pnorm(-a)
```
This a .91% confidence interval. Which is not bad, but it's not the 95% we set out for. 

```{r}
N <- 625
B <- 10^5
success  <- replicate(B,{
  X <- sample(x, N, replace = TRUE)
  p_hat <- mean(X)
  ci <- p_hat + c(-0.035,0.035)
  ci[1] <= p  & ci[2] >= p
})
mean(success)
```

Assessment: If we know the percent of democrats can be as close to 50% as 48%, how large does the sample size have to be so that the margin of error is about 2%.


If you go back to the table with which we started, you notice that many polls have sample sizes between 500 and 2500. 

## Averaging polls

So why was Nate Silver so confident?

The table shown above had several polls. We can assume they were independently run. If this is the case we have access to 

```{r}
polls %>% summarise(sum(observations))
```

samples. We can aggregate these polls. We have $k=1,\dots,K$ polls each with $N_k$ observations and en estimate $\hat{p}_k$. We can infer then that for poll $k$, $N_k \hat{p}_k$ are voting for Obama. So the aggregate $\hat{p}$ is

$$
\frac{\sum_{k=1}^K \hat{p}_k N_k}{\sum_{k=1}^K N_k}
$$
which can be viewed as a weighted average:

$$
\sum_{k=1}^P w_k \hat{p}_k \mbox{ with } w_k = \frac{N_k}{\sum_{k=1}^K N_k}
$$

Let's compute this for Obama and Romney (we assume the undecided at random)

```{r}
O <- sum(polls$Obama*polls$observations)/sum(polls$observations)/100
R <- sum(polls$Romney*polls$observations)/sum(polls$observations)/100
```

and the difference is 
```{r}
round(O-R,2)
```

The margin of error for each one is:

```{r}
N <- sum(polls$observations)
round( qnorm(.975)*sqrt(O*(1-O))/sqrt(N), 3)
round( qnorm(.975)*sqrt(R*(1-R))/sqrt(N), 3)
```

Now we have two confidence intervals?  We can form a confidence interval for the difference. The difference is approximately $\hat{p} - (1-\hat{p})$ or $1-2\hat{p}$. This implies that the standard error for the difference is twice as large than for $\hat{p}$. So our confidence interval is:

```{r}
O-R + c(-2,2)*qnorm(.975)*sqrt(O*(1-O))/sqrt(N)
```

So a 95% confidence interval still didn't quite call the election for Obama (but it was close). However, as we will learn there was much more invovled in the "90% chance of winning"" prediction which we will learn next.



#### Setting the random seed

Before we continue, we briefly explain the following important line of
code:

```{r}
set.seed(1) 
```

Throughout this book, we use random number generators. This implies that many of the results presented can actually change by chance, including the correct answer to problems. One way to ensure that results do not change is by setting R's random number generation seed. For more on the topic please read the help file:

```{r,eval=FALSE}
?set.seed
```
